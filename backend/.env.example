# Backend API Configuration
# Copy this file to .env and update with your actual values

# Server Configuration
PORT=3001
NODE_ENV=development

# LLM API Configuration (Example: Hugging Face with Llama)
LLM_API_URL=https://router.huggingface.co/featherless-ai/v1/completions
LLM_API_TOKEN=your_huggingface_token_here
LLM_DEFAULT_MODEL=meta-llama/Meta-Llama-3-8B
LLM_MAX_TOKENS=100
LLM_TEMPERATURE=0.7
LLM_REQUEST_TIMEOUT=30000
LLM_ENABLE_LOGGING=true

# Additional Examples:
# For local Ollama:
# LLM_API_URL=http://localhost:11434/api/generate
# LLM_API_TOKEN=
# LLM_DEFAULT_MODEL=llama2

# For OpenAI-compatible APIs:
# LLM_API_URL=https://api.openai.com/v1/chat/completions
# LLM_API_TOKEN=sk-your-api-key-here
# LLM_DEFAULT_MODEL=gpt-3.5-turbo
